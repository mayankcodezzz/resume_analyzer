1. **STEP 1: Project Setup and Initial Git Repository**
GOAL : Set up the project structure and initialize a Git repository.

Project Setup and Initial Git Repository
1. Create the Project Directory and Clone the Empty Repo
 - git clone "URL"
2. Get in the project Directory 
 - cd resume_analyzer
3. Open Project in Vs code 
 - code . 

Create the Directory Structure
- This structure separates our app logic (main), reusable libraries (lib), utilities (utils), and data files (data).
- Use the terminal or your editor to create the folder structure:
resume_analyzer/
├── main/
│   └── app.py
├── lib/
│   ├── groq_handler.py
│   ├── resume_analyzer.py
│   └── text_processor.py
├── utils/
│   ├── config.py
│   ├── logger.py
│   ├── file_utils.py
│   └── prompt_loader.py
├── data/
│   ├── logs/
│   └── prompts.json
├── .env
├── requirements.txt
└── README.md 

Add a README
    # Resume Analyzer
    A Streamlit app to analyze resumes using the Groq API.

Initialize Git
    git add README.md
    git status
    git commit -m "Initial commit: Project setup with README"
    git push origin main


2. **STEP 2: Set Up Dependencies**
GOAL : Install required packages and configure the environment.

Edit requirements.txt
streamlit
groq
pymupdf
python-docx
python-dotenv

Install Dependencies
- pip install -r requirements.txt

Set Up .env for the Groq API Key Go to https://console.groq.com/keys, sign up/login, 
and generate an API key. Paste it here. The .env file keeps secrets safe.
- GROQ_API_KEY=your_key_here
- Add .env to .gitignore

Commit
We’ve installed dependencies to build our app and secured our API key. Never commit .env to GitHub!
- git add requirements.txt .gitignore
- git commit -m "Add dependencies and environment setup"
- git push origin main


3. **STEP 3: Basic Streamlit App for File Upload**
GOAL : Create a simple Streamlit app to upload a resume and display its text.

*Edit main/app.py*
import streamlit as st
def main():
    st.title("Resume Analyzer")
    st.subheader("Upload a PDF or Word Resume")
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    if uploaded_file:
        st.write("File uploaded:", uploaded_file.name)
        # Placeholder for text extraction
        st.write("Text extraction coming soon!")

if __name__ == "__main__":
    main()

*Run the App*
    In the terminal: streamlit run main/app.py
    Show the UI: "You’ll see a title, subtitle, and file uploader. Upload a PDF or DOCX to see the filename."

*Commit*
- git add main/app.py
- git commit -m "Add basic Streamlit app for file upload"
- git push origin main


4. **STEP 4: Extract Text from Uploaded Files**
GOAL : Add text extraction for PDF and DOCX files.

*Edit lib/text_processor.py*
import fitz  # type: ignore # PyMuPDF - Library for handling PDF files
from docx import Document  # Library for handling Word (.docx) files

class TextProcessor:
    """A class to process and extract text from PDF and DOCX files."""
    
    def __init__(self):
        """Initialize the TextProcessor class.
        Currently no initialization parameters are needed."""
        pass

    def extract_text_from_pdf(self, pdf_path):
        """Extract text content from a PDF file.
        
        Args:
            pdf_path (str): Path to the PDF file to be processed
            
        Returns:
            str: Extracted text from all pages joined by newlines
        """
        # Open the PDF file using PyMuPDF
        doc = fitz.open(pdf_path)
        # Extract text from each page and join with newline characters
        text = "\n".join([page.get_text("text") for page in doc])
        # Close the document to free up resources
        doc.close()
        return text

    def extract_text_from_docx(self, docx_path):
        """Extract text content from a DOCX file.
        
        Args:
            docx_path (str): Path to the DOCX file to be processed
            
        Returns:
            str: Extracted text from all paragraphs joined by newlines
        """
        # Open the DOCX file using python-docx
        doc = Document(docx_path)
        # Extract text from each paragraph and join with newline characters
        text = "\n".join([para.text for para in doc.paragraphs])
        return text

    def extract_text(self, file_path, file_extension):
        """Extract text from a file based on its extension.
        
        Args:
            file_path (str): Path to the file to be processed
            file_extension (str): File extension (without dot) to determine processing method
            
        Returns:
            str: Extracted text from the file, empty string if extension not supported
        """
        # Handle PDF files
        if file_extension == "pdf":
            return self.extract_text_from_pdf(file_path)
        # Handle DOCX files
        elif file_extension == "docx":
            return self.extract_text_from_docx(file_path)
        # Return empty string for unsupported file types
        return ""

*Update main/app.py*
import os  # Operating system interfaces for file handling
import sys
# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import streamlit as st  # Web app framework for creating interactive interfaces
from lib.text_processor import TextProcessor  # Custom class for text extraction

def main():
    """Main function to run the Streamlit Resume Analyzer application."""
    # Set the title of the web application
    st.title("Resume Analyzer")
    # Add a subheader to provide context
    st.subheader("Upload a PDF or Word Resume")
    
    # Create a file uploader widget accepting PDF and DOCX files
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    
    # Check if a file is uploaded and extract button is clicked
    if uploaded_file and st.button("Extract Text"):
        # Get the file extension from the uploaded file name
        file_extension = uploaded_file.name.split(".")[-1].lower()
        # Define a temporary file path for processing
        temp_path = f"temp_resume.{file_extension}"
        
        # Save the uploaded file temporarily to disk
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # Create an instance of TextProcessor to handle text extraction
        processor = TextProcessor()
        # Extract text using the appropriate method based on file extension
        extracted_text = processor.extract_text(temp_path, file_extension)
        
        # Clean up by removing the temporary file
        os.remove(temp_path)
        
        # Display the extracted text section
        st.write("Extracted Text:")
        # Show the extracted text in a scrollable text area
        st.text_area("Text", extracted_text, height=300)
# Entry point of the script - runs the application when executed directly.
if __name__ == "__main__":
    main()

*Test*
    - Run streamlit run main/app.py, upload a file, and click "Extract Text" to see the output.

*Commit*
- git add lib/text_processor.py main/app.py
- git commit -m "Add text extraction for PDF and DOCX files"
- git push origin main


5. **STEP 5: Set Up Groq API Integration**
GOAL : Connect to the Groq API to analyze text.

*Edit utils/config.py*
import os  # Operating system interfaces for environment variable access
from dotenv import load_dotenv  # Library to load environment variables from a .env file

# Load environment variables from .env file into the application's environment
load_dotenv()

class Config:
    """Configuration class to manage application settings and environment variables."""
    
    # Static attribute to store the Grok API key retrieved from environment variables
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")

    @staticmethod
    def validate():
        """Validate that required environment variables are set.
        
        Raises:
            ValueError: If GROQ_API_KEY is not set or is empty
        """
        # Check if GROQ_API_KEY is missing or empty
        if not Config.GROQ_API_KEY:
            raise ValueError("GROQ_API_KEY is not set in the .env file")

*Edit lib/groq_handler.py*
from groq import Groq  # Groq SDK for interacting with the Groq API
from utils.config import Config  # Custom configuration class for environment settings

class GroqHandler:
    """Handler class for managing interactions with the Groq API."""
    
    def __init__(self):
        """Initialize the GroqHandler with a validated API client.
        
        Raises:
            ValueError: If Config validation fails (e.g., missing API key)
        """
        # Validate configuration settings before proceeding
        Config.validate()
        # Initialize Groq client with the API key from Config
        self.client = Groq(api_key=Config.GROQ_API_KEY)

    def analyze_text(self, prompt, text, model="gemma2-9b-it", max_tokens=2000):
        """Analyze text using the Groq API with a specified prompt and model.
        
        Args:
            prompt (str): The instruction or question to guide the analysis
            text (str): The text content to be analyzed
            model (str): The Groq model to use (default: "gemma2-9b-it")
            max_tokens (int): Maximum number of tokens in the response (default: 2000)
            
        Returns:
            str: The analyzed text response from the Groq API, stripped of leading/trailing whitespace
        """
        # Send a request to the Groq API with the combined prompt and text
        response = self.client.chat.completions.create(
            messages=[{"role": "user", "content": prompt + "\n\n" + text}],
            model=model,
            max_tokens=max_tokens
        )
        # Extract and return the content from the first response choice
        return response.choices[0].message.content.strip()

*Update main/app.py*
import os  # Operating system interfaces for file handling
import sys
# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import streamlit as st  # Web app framework for creating interactive interfaces
from lib.text_processor import TextProcessor  # Custom class for text extraction
from lib.groq_handler import GroqHandler  # Custom class for interacting with Groq API

def main():
    """Main function to run the Streamlit Resume Analyzer application."""
    # Set the title of the web application
    st.title("Resume Analyzer")
    # Add a subheader to provide context for the user
    st.subheader("Upload a PDF or Word Resume")
    
    # Create a file uploader widget accepting PDF and DOCX files
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    
    # Check if a file is uploaded and the Analyze button is clicked
    if uploaded_file and st.button("Analyze"):
        # Extract the file extension from the uploaded file name
        file_extension = uploaded_file.name.split(".")[-1].lower()
        # Define a temporary file path for processing
        temp_path = f"temp_resume.{file_extension}"
        
        # Save the uploaded file temporarily to disk in binary write mode
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # Instantiate TextProcessor to extract text from the file
        processor = TextProcessor()
        # Extract text based on the file type
        extracted_text = processor.extract_text(temp_path, file_extension)
        
        # Remove the temporary file to clean up
        os.remove(temp_path)
        
        # Proceed if text was successfully extracted
        if extracted_text:
            # Instantiate GroqHandler for text analysis
            groq = GroqHandler()
            # Define the prompt for Groq API analysis
            prompt = "Analyze this resume text and summarize its key points."
            
            # Display a spinner while analysis is in progress
            with st.spinner("Analyzing..."):
                # Analyze the extracted text using Groq API
                analysis = groq.analyze_text(prompt, extracted_text)
            
            # Display the analysis header
            st.write("Analysis:")
            # Render the analysis result as markdown for better formatting
            st.markdown(analysis)
        else:
            # Display an error message if no text was extracted
            st.error("No text extracted from the file.")

# Entry point of the script - runs the application when executed directly.
if __name__ == "__main__":
    main()

*Test*
    - Run the streamlit run main/app.py, upload a resume, and click "Analyze" to see a basic summary.

*Commit*
- git add utils/config.py lib/groq_handler.py main/app.py
- git commit -m "Integrate Groq API for basic text analysis"
- git push origin main


6. **STEP 6: Add Structured Analysis with Prompts**
GOAL : Use prompt_loader.py and resume_analyzer.py for structured output.

*Update utils/config.py*
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Config:
    """Configuration class for the application."""
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")
    LOG_DIR = "data/logs"
    PROMPTS_FILE = "data/prompts.json"

    @staticmethod
    def validate():
        """Validate that required environment variables are set."""
        if not Config.GROQ_API_KEY:
            raise ValueError("GROQ_API_KEY is not set in the .env file")

*Edit data/prompts.json*
{
  "resume_analysis": {
    "description": "Prompt for analyzing a resume based on designation, experience, and domain",
    "template": "Analyze the following resume text for this context:\n- Desired Designation: {designation}\n- Experience Level: {experience}\n- Domain: {domain}\n\nProvide three sections based ONLY on the explicit resume content:\n1. Strengths: List in bullet points using \"Your\" (Education, Work Experience, Skills, Projects, Certifications, writing style).\n2. Areas to Improve: List in bullet points using \"Your\". If none, say \"- No significant improvements identified.\"\n3. Score: Provide ONE score out of 100 as \"Score: [number]\".\n\nFormat:\nStrengths:\n- Your strength 1\n- Your strength 2\nAreas to Improve:\n- Your area 1\n- Your area 2\nScore: [number]"
  },
  "combine_partial_responses": {
    "description": "Prompt for combining multiple partial responses into a coherent one",
    "template": "Combine multiple partial responses into one coherent response.\nPreserve all details and remove duplicates or conflicts."
  }
}

*Edit utils/prompt_loader.py*
import json
import os

class PromptLoader:
    """Class to load and format prompts from a JSON file."""
    def __init__(self, prompts_file):
        self.prompts_file = prompts_file
        self.prompts = self._load_prompts()

    def _load_prompts(self):
        """Load prompts from the JSON file."""
        with open(self.prompts_file, "r", encoding="utf-8") as f:
            prompts = json.load(f)
        return prompts

    def get_prompt(self, prompt_key, **kwargs):
        """Fetch and format a prompt by key with provided variables."""
        template = self.prompts[prompt_key]["template"]
        return template.format(**kwargs)
    
*Edit lib/resume_analyzer.py*
class ResumeAnalyzer:
    """Class to analyze resumes using GroqHandler."""
    def __init__(self, groq_handler, prompt_loader):
        self.grok = groq_handler
        self.prompt_loader = prompt_loader

    def analyze_resume(self, text, designation, experience, domain):
        """Analyze resume text."""
        prompt = self.prompt_loader.get_prompt(
            "resume_analysis",
            designation=designation,
            experience=experience,
            domain=domain
        )
        result = self.grok.analyze_text(prompt, text, max_tokens=1500)
        return result
    
*Update main/app.py*
import sys
import os
import streamlit as st

# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from lib.groq_handler import GroqHandler
from lib.text_processor import TextProcessor
from lib.resume_analyzer import ResumeAnalyzer
from utils.config import Config
from utils.prompt_loader import PromptLoader

def main():
    """Main function to run the Streamlit Resume Analyzer app."""
    st.title("Resume Analyzer")
    st.subheader("Upload a PDF or Word Resume")
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    col1, col2, col3 = st.columns(3)
    with col1:
        designation = st.selectbox("Select Desired Designation", ["Data Scientist", "Data Analyst", "MLOps Engineer", "Machine Learning Engineer"])
    with col2:
        experience = st.selectbox("Select Experience Level", ["Fresher", "<1 Year Experience", "1-2 Years Experience", "2-5 Years Experience", "5-8 Years Experience", "8-10 Years Experience"])
    with col3:
        domain = st.selectbox("Select Domain", ["Finance", "Healthcare", "Automobile", "Real Estate"])
    if uploaded_file and st.button("Analyze"):
        file_extension = uploaded_file.name.split(".")[-1].lower()
        temp_path = f"temp_resume.{file_extension}"
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        text_processor = TextProcessor()
        extracted_text = text_processor.extract_text(temp_path, file_extension)
        os.remove(temp_path)
        if extracted_text:
            grok_handler = GroqHandler()
            prompt_loader = PromptLoader(Config.PROMPTS_FILE)
            resume_analyzer = ResumeAnalyzer(grok_handler, prompt_loader)
            with st.spinner("Analyzing resume... Please wait"):
                analysis = resume_analyzer.analyze_resume(extracted_text, designation, experience, domain)
            st.markdown("# Resume Analysis")
            st.write(analysis)
        else:
            st.error("Could not extract text.")

if __name__ == "__main__":
    main()

*Test*
    Run streamlit run main/app.py and test with dropdown selections.

*Commit*
- git add utils/config.py data/prompts.json utils/prompt_loader.py lib/resume_analyzer.py main/app.py
- git commit -m "Add structured analysis with prompts and update Config"
- git push origin main


7. **STEP 7: Add Logging and Exception Handling**
GOAL : Enhance with logger.py and error handling.

*Edit utils/logger.py*
import logging
import os

def setup_logger(name, log_file, level=logging.DEBUG):
    """Set up a logger with console and file handlers."""
    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Ensure logs directory exists
    log_dir = os.path.dirname(log_file)
    os.makedirs(log_dir, exist_ok=True)

    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)

    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(level)

    # Formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    # Add handlers (only if not already added)
    if not logger.handlers:
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

    return logger

*Update lib/text_processor.py*
import fitz  # PyMuPDF
from docx import Document

class TextProcessor:
    """Class to extract text from PDF and DOCX files."""
    def __init__(self, logger):
        self.logger = logger

    def extract_text_from_pdf(self, pdf_path):
        """Extract text from a PDF file."""
        self.logger.debug("Extracting text from PDF: %s", pdf_path)
        try:
            doc = fitz.open(pdf_path)
            text = "\n".join([page.get_text("text") for page in doc])
            doc.close()
            self.logger.debug("Text extracted from PDF: %s", pdf_path)
            return text
        except Exception as e:
            self.logger.error("Error extracting text from PDF %s: %s", pdf_path, str(e))
            raise Exception(f"Error extracting text from PDF: {str(e)}")

    def extract_text_from_docx(self, docx_path):
        """Extract text from a DOCX file."""
        self.logger.debug("Extracting text from DOCX: %s", docx_path)
        try:
            doc = Document(docx_path)
            text = "\n".join([para.text for para in doc.paragraphs])
            self.logger.debug("Text extracted from DOCX: %s", docx_path)
            return text
        except Exception as e:
            self.logger.error("Error extracting text from DOCX %s: %s", docx_path, str(e))
            raise Exception(f"Error extracting text from DOCX: {str(e)}")

    def extract_text(self, file_path, file_extension):
        """Extract text based on file extension."""
        self.logger.debug("Extracting text from file: %s", file_path)
        if file_extension == "pdf":
            return self.extract_text_from_pdf(file_path)
        elif file_extension == "docx":
            return self.extract_text_from_docx(file_path)
        self.logger.warning("Unsupported file extension: %s", file_extension)
        return ""

*Update lib/groq_handler.py*
from groq import Groq
from utils.config import Config

class GroqHandler:
    """Class to handle interactions with the Groq API."""
    def __init__(self, logger, prompt_loader):
        self.logger = logger
        self.prompt_loader = prompt_loader
        self.logger.debug("Initializing GroqHandler")
        try:
            Config.validate()  # Ensure API key is set
            self.client = Groq(api_key=Config.GROQ_API_KEY)
            self.logger.debug("GroqHandler initialized successfully")
        except Exception as e:
            self.logger.error("Error initializing Groq client: %s", str(e))
            raise ValueError(f"Error initializing Groq client: {str(e)}")

    def analyze_text(self, prompt, text, model="gemma2-9b-it", max_tokens=2000, temperature=0):
        """Analyze text using the Groq API."""
        self.logger.debug("Starting text analysis with model: %s, max_tokens: %d", model, max_tokens)
        try:
            max_length = 3000
            chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]
            partial_responses = []

            for i, chunk in enumerate(chunks):
                self.logger.debug("Processing chunk %d of %d", i + 1, len(chunks))
                response = self.client.chat.completions.create(
                    messages=[{"role": "user", "content": prompt + "\n\n" + chunk}],
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                partial_responses.append(response.choices[0].message.content.strip())
                self.logger.debug("Chunk %d processed", i + 1)

            if len(partial_responses) > 1:
                self.logger.debug("Combining %d partial responses", len(partial_responses))
                combine_prompt = self.prompt_loader.get_prompt("combine_partial_responses")
                combined_text = "\n\n".join(partial_responses)
                final_response = self.client.chat.completions.create(
                    messages=[{"role": "user", "content": combine_prompt + "\n\n" + combined_text}],
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                result = final_response.choices[0].message.content.strip()
            else:
                result = partial_responses[0]

            self.logger.debug("Text analysis completed")
            return result
        except Exception as e:
            self.logger.error("Error processing text with Groq API: %s", str(e))
            raise Exception(f"Error processing text with Groq API: {str(e)}")

*lib/resume_analyzer.py*
class ResumeAnalyzer:
    """Class to analyze resumes using GroqHandler."""
    def __init__(self, groq_handler, logger, prompt_loader):
        self.grok = groq_handler
        self.logger = logger
        self.prompt_loader = prompt_loader

    def analyze_resume(self, text, designation, experience, domain):
        """Analyze resume text."""
        self.logger.debug("Starting resume analysis for designation: %s, experience: %s, domain: %s",
                         designation, experience, domain)
        try:
            prompt = self.prompt_loader.get_prompt(
                "resume_analysis",
                designation=designation,
                experience=experience,
                domain=domain
            )
            result = self.grok.analyze_text(prompt, text, max_tokens=1500)
            self.logger.debug("Resume analysis completed")
            return result
        except Exception as e:
            self.logger.error("Error analyzing resume: %s", str(e))
            raise Exception(f"Error analyzing resume: {str(e)}")
    
*utils/prompt_loader.py*
import json
import os

class PromptLoader:
    """Class to load and format prompts from a JSON file."""
    def __init__(self, prompts_file, logger):
        self.logger = logger
        self.prompts_file = prompts_file
        self.prompts = self._load_prompts()

    def _load_prompts(self):
        """Load prompts from the JSON file."""
        try:
            self.logger.debug("Loading prompts from %s", self.prompts_file)
            if not os.path.exists(self.prompts_file):
                self.logger.error("Prompts file not found: %s", self.prompts_file)
                raise FileNotFoundError(f"Prompts file not found: {self.prompts_file}")
            with open(self.prompts_file, "r", encoding="utf-8") as f:
                prompts = json.load(f)
            self.logger.debug("Prompts loaded successfully")
            return prompts
        except Exception as e:
            self.logger.error("Error loading prompts: %s", str(e))
            raise Exception(f"Error loading prompts: {str(e)}")

    def get_prompt(self, prompt_key, **kwargs):
        """Fetch and format a prompt by key with provided variables."""
        try:
            if prompt_key not in self.prompts:
                self.logger.error("Prompt key not found: %s", prompt_key)
                raise KeyError(f"Prompt key not found: {prompt_key}")
            template = self.prompts[prompt_key]["template"]
            formatted_prompt = template.format(**kwargs)
            self.logger.debug("Prompt fetched and formatted for key: %s", prompt_key)
            return formatted_prompt
        except Exception as e:
            self.logger.error("Error formatting prompt %s: %s", prompt_key, str(e))
            raise Exception(f"Error formatting prompt {prompt_key}: {str(e)}")

*Update main/app.py*
import sys
import os
import streamlit as st

# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from lib.groq_handler import GroqHandler
from lib.text_processor import TextProcessor
from lib.resume_analyzer import ResumeAnalyzer
from utils.config import Config
from utils.logger import setup_logger
from utils.prompt_loader import PromptLoader

# Singleton logger setup (runs only once)
if 'loggers' not in st.session_state:
    Config.validate()  # Validate config once
    st.session_state.loggers = {
        "app": setup_logger("app", f"{Config.LOG_DIR}/app.log"),
        "groq_handler": setup_logger("groq_handler", f"{Config.LOG_DIR}/groq_handler.log"),
        "prompt_loader": setup_logger("prompt_loader", f"{Config.LOG_DIR}/prompt_loader.log"),
        "resume_analyzer": setup_logger("resume_analyzer", f"{Config.LOG_DIR}/resume_analyzer.log"),
        "text_processor": setup_logger("text_processor", f"{Config.LOG_DIR}/text_processor.log")
    }
    st.session_state.loggers["app"].debug("Starting Resume Analyzer application")

loggers = st.session_state.loggers

def main():
    """Main function to run the Streamlit Resume Analyzer app."""
    st.title("Resume Analyzer")
    st.subheader("Upload a PDF or Word Resume")
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    col1, col2, col3 = st.columns(3)
    with col1:
        designation = st.selectbox("Select Desired Designation", ["Data Scientist", "Data Analyst", "MLOps Engineer", "Machine Learning Engineer"])
    with col2:
        experience = st.selectbox("Select Experience Level", ["Fresher", "<1 Year Experience", "1-2 Years Experience", "2-5 Years Experience", "5-8 Years Experience", "8-10 Years Experience"])
    with col3:
        domain = st.selectbox("Select Domain", ["Finance", "Healthcare", "Automobile", "Real Estate"])
    if st.button("Analyze") and uploaded_file:
        loggers["app"].debug("User clicked Analyze button for file: %s", uploaded_file.name)
        file_extension = uploaded_file.name.split(".")[-1].lower()
        temp_path = f"temp_resume.{file_extension}"
        try:
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            text_processor = TextProcessor(loggers["text_processor"])
            extracted_text = text_processor.extract_text(temp_path, file_extension)
            os.remove(temp_path)
            if extracted_text:
                grok_handler = GroqHandler(loggers["groq_handler"], PromptLoader(Config.PROMPTS_FILE, loggers["prompt_loader"]))
                resume_analyzer = ResumeAnalyzer(grok_handler, loggers["resume_analyzer"], PromptLoader(Config.PROMPTS_FILE, loggers["prompt_loader"]))
                with st.spinner("Analyzing resume... Please wait"):
                    analysis = resume_analyzer.analyze_resume(extracted_text, designation, experience, domain)
                st.markdown("# Resume Analysis")
                st.write(analysis)
            else:
                st.error("Could not extract text.")
                loggers["app"].error("Failed to extract text from %s", uploaded_file.name)
        except Exception as e:
            loggers["app"].error("Error processing resume: %s", str(e))
            st.error(f"Error processing resume: {str(e)}")
            if os.path.exists(temp_path):
                os.remove(temp_path)

if __name__ == "__main__":
    main()

*Test*
Run streamlit run main/app.py and check logs in data/logs/.

*Commit*
- git add utils/logger.py lib/text_processor.py lib/groq_handler.py lib/resume_analyzer.py utils/prompt_loader.py main/app.py
- git commit -m "Add logging and exception handling"
- git push origin main


8. **STEP 8: Final Enhancements (Caching, Download, Multi-Page)**
GOAL : Add Caching, Download, Multi-Page.

*Edit utils/file_utils.py*
import os

def save_text_to_file(text, output_path):
    """Save text to a file with UTF-8 encoding."""
    with open(output_path, "w", encoding="utf-8") as file:
        file.write(text)

def remove_file(file_path):
    """Remove a file if it exists."""
    if os.path.exists(file_path):
        os.remove(file_path)

*Update main/app.py*
import sys
import os
import streamlit as st

# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from lib.groq_handler import GroqHandler
from lib.text_processor import TextProcessor
from lib.resume_analyzer import ResumeAnalyzer
from utils.file_utils import save_text_to_file, remove_file
from utils.logger import setup_logger
from utils.config import Config
from utils.prompt_loader import PromptLoader

# Singleton logger setup (runs only once)
if 'loggers' not in st.session_state:
    Config.validate()  # Validate config once
    st.session_state.loggers = {
        "app": setup_logger("app", f"{Config.LOG_DIR}/app.log"),
        "groq_handler": setup_logger("groq_handler", f"{Config.LOG_DIR}/groq_handler.log"),
        "prompt_loader": setup_logger("prompt_loader", f"{Config.LOG_DIR}/prompt_loader.log"),
        "resume_analyzer": setup_logger("resume_analyzer", f"{Config.LOG_DIR}/resume_analyzer.log"),
        "text_processor": setup_logger("text_processor", f"{Config.LOG_DIR}/text_processor.log")
    }
    st.session_state.loggers["app"].debug("Starting Resume Analyzer application")

loggers = st.session_state.loggers

# Cache components to prevent re-initialization
@st.cache_resource
def get_grok_handler():
    return GroqHandler(loggers["groq_handler"], PromptLoader(Config.PROMPTS_FILE, loggers["prompt_loader"]))

@st.cache_resource
def get_text_processor():
    return TextProcessor(loggers["text_processor"])

@st.cache_resource
def get_resume_analyzer(_grok_handler):
    return ResumeAnalyzer(_grok_handler, loggers["resume_analyzer"], PromptLoader(Config.PROMPTS_FILE, loggers["prompt_loader"]))

# Initialize components (cached)
grok_handler = get_grok_handler()
text_processor = get_text_processor()
resume_analyzer = get_resume_analyzer(grok_handler)

def main():
    """Main function to run the Streamlit Resume Analyzer app."""
    if 'page' not in st.session_state:
        st.session_state.page = "upload"
    if 'analysis' not in st.session_state:
        st.session_state.analysis = None
    if 'processed' not in st.session_state:
        st.session_state.processed = False

    if st.session_state.page == "upload":
        st.title("Resume Analyzer")
        st.subheader("Upload a PDF or Word Resume")

        uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
        col1, col2, col3 = st.columns(3)
        with col1:
            designation = st.selectbox("Select Desired Designation", ["Data Scientist", "Data Analyst", "MLOps Engineer", "Machine Learning Engineer"])
        with col2:
            experience = st.selectbox("Select Experience Level", ["Fresher", "<1 Year Experience", "1-2 Years Experience", "2-5 Years Experience", "5-8 Years Experience", "8-10 Years Experience"])
        with col3:
            domain = st.selectbox("Select Domain", ["Finance", "Healthcare", "Automobile", "Real Estate"])

        if st.button("Analyze") and uploaded_file:
            loggers["app"].debug("User clicked Analyze button for file: %s", uploaded_file.name)
            st.session_state.uploaded_file = uploaded_file
            st.session_state.designation = designation
            st.session_state.experience = experience
            st.session_state.domain = domain
            st.session_state.page = "results"
            st.session_state.processed = False
            st.rerun()

    elif st.session_state.page == "results":
        uploaded_file = st.session_state.uploaded_file
        file_extension = uploaded_file.name.split(".")[-1].lower()
        temp_path = f"temp_resume.{file_extension}"

        try:
            if not st.session_state.processed:  # Process only once
                loggers["app"].debug("Processing uploaded file: %s", uploaded_file.name)
                with open(temp_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                extracted_text = text_processor.extract_text(temp_path, file_extension)
                remove_file(temp_path)

                if extracted_text:
                    loggers["app"].debug("Text extracted successfully, length: %d characters", len(extracted_text))
                    with st.spinner("Analyzing resume... Please wait"):
                        st.session_state.analysis = resume_analyzer.analyze_resume(
                            extracted_text, st.session_state.designation, st.session_state.experience, st.session_state.domain
                        )
                        st.session_state.processed = True
                    loggers["app"].debug("Resume analysis completed")
                else:
                    st.error("Could not extract text. Please check the file format.")
                    loggers["app"].error("Failed to extract text from %s", uploaded_file.name)

            if st.session_state.analysis:
                if st.button("Upload New Resume"):
                    loggers["app"].debug("User clicked Upload New Resume")
                    st.session_state.page = "upload"
                    st.session_state.analysis = None
                    st.session_state.processed = False
                    st.rerun()

                st.markdown("# Resume Analysis")
                st.write(st.session_state.analysis)

                output_filename = "resume_analysis.txt"
                save_text_to_file(st.session_state.analysis, output_filename)
                with open(output_filename, "rb") as file:
                    st.download_button(label="Download Analysis", data=file, file_name=output_filename, mime="text/plain")
                remove_file(output_filename)

        except Exception as e:
            loggers["app"].error("Error processing resume: %s", str(e))
            st.error(f"Error processing resume: {str(e)}")
            remove_file(temp_path)

if __name__ == "__main__":
    main()

*Test*
    Run streamlit run main/app.py and verify multi-page navigation, caching, and download functionality.

*Commit*
- git add utils/file_utils.py main/app.py
- git commit -m "Final enhancements: caching, download, and multi-page navigation"
- git push origin main
1. **STEP 1: Project Setup and Initial Git Repository**
GOAL : Set up the project structure and initialize a Git repository.

Project Setup and Initial Git Repository
1. Create the Project Directory and Clone the Empty Repo
 - git clone "URL"
2. Get in the project Directory 
 - cd resume_analyzer
3. Open Project in Vs code 
 - code . 

Create the Directory Structure
- This structure separates our app logic (main), reusable libraries (lib), utilities (utils), and data files (data).
- Use the terminal or your editor to create the folder structure:
resume_analyzer/
├── main/
│   └── app.py
├── lib/
│   ├── groq_handler.py
│   ├── resume_analyzer.py
│   └── text_processor.py
├── utils/
│   ├── config.py
│   ├── logger.py
│   ├── file_utils.py
│   └── prompt_loader.py
├── data/
│   ├── logs/
│   └── prompts.json
├── .env
├── requirements.txt
└── README.md 

Add a README
    # Resume Analyzer
    A Streamlit app to analyze resumes using the Groq API.

Initialize Git
    git add README.md
    git status
    git commit -m "Initial commit: Project setup with README"
    git push origin main


2. **STEP 2: Set Up Dependencies**
GOAL : Install required packages and configure the environment.

Edit requirements.txt
streamlit
groq
pymupdf
python-docx
python-dotenv

Install Dependencies
- pip install -r requirements.txt

Set Up .env for the Groq API Key Go to https://console.groq.com/keys, sign up/login, 
and generate an API key. Paste it here. The .env file keeps secrets safe.
- GROQ_API_KEY=your_key_here
- Add .env to .gitignore

Commit
We’ve installed dependencies to build our app and secured our API key. Never commit .env to GitHub!
- git add requirements.txt .gitignore
- git commit -m "Add dependencies and environment setup"
- git push origin main


3. **STEP 3: Basic Streamlit App for File Upload**
GOAL : Create a simple Streamlit app to upload a resume and display its text.

*Edit main/app.py*
import streamlit as st
def main():
    st.title("Resume Analyzer")
    st.subheader("Upload a PDF or Word Resume")
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    if uploaded_file:
        st.write("File uploaded:", uploaded_file.name)
        # Placeholder for text extraction
        st.write("Text extraction coming soon!")

if __name__ == "__main__":
    main()

*Run the App*
    In the terminal: streamlit run main/app.py
    Show the UI: "You’ll see a title, subtitle, and file uploader. Upload a PDF or DOCX to see the filename."

*Commit*
- git add main/app.py
- git commit -m "Add basic Streamlit app for file upload"
- git push origin main


4. **STEP 4: Extract Text from Uploaded Files**
GOAL : Add text extraction for PDF and DOCX files.

*Edit lib/text_processor.py*
import fitz  # type: ignore # PyMuPDF - Library for handling PDF files
from docx import Document  # Library for handling Word (.docx) files

class TextProcessor:
    """A class to process and extract text from PDF and DOCX files."""
    
    def __init__(self):
        """Initialize the TextProcessor class.
        Currently no initialization parameters are needed."""
        pass

    def extract_text_from_pdf(self, pdf_path):
        """Extract text content from a PDF file.
        
        Args:
            pdf_path (str): Path to the PDF file to be processed
            
        Returns:
            str: Extracted text from all pages joined by newlines
        """
        # Open the PDF file using PyMuPDF
        doc = fitz.open(pdf_path)
        # Extract text from each page and join with newline characters
        text = "\n".join([page.get_text("text") for page in doc])
        # Close the document to free up resources
        doc.close()
        return text

    def extract_text_from_docx(self, docx_path):
        """Extract text content from a DOCX file.
        
        Args:
            docx_path (str): Path to the DOCX file to be processed
            
        Returns:
            str: Extracted text from all paragraphs joined by newlines
        """
        # Open the DOCX file using python-docx
        doc = Document(docx_path)
        # Extract text from each paragraph and join with newline characters
        text = "\n".join([para.text for para in doc.paragraphs])
        return text

    def extract_text(self, file_path, file_extension):
        """Extract text from a file based on its extension.
        
        Args:
            file_path (str): Path to the file to be processed
            file_extension (str): File extension (without dot) to determine processing method
            
        Returns:
            str: Extracted text from the file, empty string if extension not supported
        """
        # Handle PDF files
        if file_extension == "pdf":
            return self.extract_text_from_pdf(file_path)
        # Handle DOCX files
        elif file_extension == "docx":
            return self.extract_text_from_docx(file_path)
        # Return empty string for unsupported file types
        return ""

*Update main/app.py*
import os  # Operating system interfaces for file handling
import sys
# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import streamlit as st  # Web app framework for creating interactive interfaces
from lib.text_processor import TextProcessor  # Custom class for text extraction

def main():
    """Main function to run the Streamlit Resume Analyzer application."""
    # Set the title of the web application
    st.title("Resume Analyzer")
    # Add a subheader to provide context
    st.subheader("Upload a PDF or Word Resume")
    
    # Create a file uploader widget accepting PDF and DOCX files
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    
    # Check if a file is uploaded and extract button is clicked
    if uploaded_file and st.button("Extract Text"):
        # Get the file extension from the uploaded file name
        file_extension = uploaded_file.name.split(".")[-1].lower()
        # Define a temporary file path for processing
        temp_path = f"temp_resume.{file_extension}"
        
        # Save the uploaded file temporarily to disk
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # Create an instance of TextProcessor to handle text extraction
        processor = TextProcessor()
        # Extract text using the appropriate method based on file extension
        extracted_text = processor.extract_text(temp_path, file_extension)
        
        # Clean up by removing the temporary file
        os.remove(temp_path)
        
        # Display the extracted text section
        st.write("Extracted Text:")
        # Show the extracted text in a scrollable text area
        st.text_area("Text", extracted_text, height=300)
# Entry point of the script - runs the application when executed directly.
if __name__ == "__main__":
    main()

*Test*
    - Run streamlit run main/app.py, upload a file, and click "Extract Text" to see the output.

*Commit*
- git add lib/text_processor.py main/app.py
- git commit -m "Add text extraction for PDF and DOCX files"
- git push origin main


5. **STEP 5: Set Up Groq API Integration**
GOAL : Connect to the Groq API to analyze text.

*Edit utils/config.py*
import os  # Operating system interfaces for environment variable access
from dotenv import load_dotenv  # Library to load environment variables from a .env file

# Load environment variables from .env file into the application's environment
load_dotenv()

class Config:
    """Configuration class to manage application settings and environment variables."""
    
    # Static attribute to store the Grok API key retrieved from environment variables
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")

    @staticmethod
    def validate():
        """Validate that required environment variables are set.
        
        Raises:
            ValueError: If GROQ_API_KEY is not set or is empty
        """
        # Check if GROQ_API_KEY is missing or empty
        if not Config.GROQ_API_KEY:
            raise ValueError("GROQ_API_KEY is not set in the .env file")

*Edit lib/groq_handler.py*
from groq import Groq  # Groq SDK for interacting with the Groq API
from utils.config import Config  # Custom configuration class for environment settings

class GroqHandler:
    """Handler class for managing interactions with the Groq API."""
    
    def __init__(self):
        """Initialize the GroqHandler with a validated API client.
        
        Raises:
            ValueError: If Config validation fails (e.g., missing API key)
        """
        # Validate configuration settings before proceeding
        Config.validate()
        # Initialize Groq client with the API key from Config
        self.client = Groq(api_key=Config.GROQ_API_KEY)

    def analyze_text(self, prompt, text, model="gemma2-9b-it", max_tokens=2000):
        """Analyze text using the Groq API with a specified prompt and model.
        
        Args:
            prompt (str): The instruction or question to guide the analysis
            text (str): The text content to be analyzed
            model (str): The Groq model to use (default: "gemma2-9b-it")
            max_tokens (int): Maximum number of tokens in the response (default: 2000)
            
        Returns:
            str: The analyzed text response from the Groq API, stripped of leading/trailing whitespace
        """
        # Send a request to the Groq API with the combined prompt and text
        response = self.client.chat.completions.create(
            messages=[{"role": "user", "content": prompt + "\n\n" + text}],
            model=model,
            max_tokens=max_tokens
        )
        # Extract and return the content from the first response choice
        return response.choices[0].message.content.strip()

*Update main/app.py*
import os  # Operating system interfaces for file handling
import sys
# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import streamlit as st  # Web app framework for creating interactive interfaces
from lib.text_processor import TextProcessor  # Custom class for text extraction
from lib.groq_handler import GroqHandler  # Custom class for interacting with Groq API

def main():
    """Main function to run the Streamlit Resume Analyzer application."""
    # Set the title of the web application
    st.title("Resume Analyzer")
    # Add a subheader to provide context for the user
    st.subheader("Upload a PDF or Word Resume")
    
    # Create a file uploader widget accepting PDF and DOCX files
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    
    # Check if a file is uploaded and the Analyze button is clicked
    if uploaded_file and st.button("Analyze"):
        # Extract the file extension from the uploaded file name
        file_extension = uploaded_file.name.split(".")[-1].lower()
        # Define a temporary file path for processing
        temp_path = f"temp_resume.{file_extension}"
        
        # Save the uploaded file temporarily to disk in binary write mode
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # Instantiate TextProcessor to extract text from the file
        processor = TextProcessor()
        # Extract text based on the file type
        extracted_text = processor.extract_text(temp_path, file_extension)
        
        # Remove the temporary file to clean up
        os.remove(temp_path)
        
        # Proceed if text was successfully extracted
        if extracted_text:
            # Instantiate GroqHandler for text analysis
            groq = GroqHandler()
            # Define the prompt for Groq API analysis
            prompt = "Analyze this resume text and summarize its key points."
            
            # Display a spinner while analysis is in progress
            with st.spinner("Analyzing..."):
                # Analyze the extracted text using Groq API
                analysis = groq.analyze_text(prompt, extracted_text)
            
            # Display the analysis header
            st.write("Analysis:")
            # Render the analysis result as markdown for better formatting
            st.markdown(analysis)
        else:
            # Display an error message if no text was extracted
            st.error("No text extracted from the file.")

# Entry point of the script - runs the application when executed directly.
if __name__ == "__main__":
    main()

*Test*
    - Run the streamlit run main/app.py, upload a resume, and click "Analyze" to see a basic summary.

*Commit*
- git add utils/config.py lib/groq_handler.py main/app.py
- git commit -m "Integrate Groq API for basic text analysis"
- git push origin main


6. **STEP 6: Add Structured Analysis with Prompts**
GOAL : Use prompt_loader.py and resume_analyzer.py for structured output.

*Update utils/config.py*
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Config:
    """Configuration class for the application."""
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")
    LOG_DIR = "data/logs"
    PROMPTS_FILE = "data/prompts.json"

    @staticmethod
    def validate():
        """Validate that required environment variables are set."""
        if not Config.GROQ_API_KEY:
            raise ValueError("GROQ_API_KEY is not set in the .env file")

*Edit data/prompts.json*
{
  "resume_analysis": {
    "description": "Prompt for analyzing a resume based on designation, experience, and domain",
    "template": "Analyze the following resume text for this context:\n- Desired Designation: {designation}\n- Experience Level: {experience}\n- Domain: {domain}\n\nProvide three sections based ONLY on the explicit resume content:\n1. Strengths: List in bullet points using \"Your\" (Education, Work Experience, Skills, Projects, Certifications, writing style).\n2. Areas to Improve: List in bullet points using \"Your\". If none, say \"- No significant improvements identified.\"\n3. Score: Provide ONE score out of 100 as \"Score: [number]\".\n\nFormat:\nStrengths:\n- Your strength 1\n- Your strength 2\nAreas to Improve:\n- Your area 1\n- Your area 2\nScore: [number]"
  },
  "combine_partial_responses": {
    "description": "Prompt for combining multiple partial responses into a coherent one",
    "template": "Combine multiple partial responses into one coherent response.\nPreserve all details and remove duplicates or conflicts."
  }
}

*Edit utils/prompt_loader.py*
import json
import os

class PromptLoader:
    """Class to load and format prompts from a JSON file."""
    def __init__(self, prompts_file):
        self.prompts_file = prompts_file
        self.prompts = self._load_prompts()

    def _load_prompts(self):
        """Load prompts from the JSON file."""
        with open(self.prompts_file, "r", encoding="utf-8") as f:
            prompts = json.load(f)
        return prompts

    def get_prompt(self, prompt_key, **kwargs):
        """Fetch and format a prompt by key with provided variables."""
        template = self.prompts[prompt_key]["template"]
        return template.format(**kwargs)
    
*Edit lib/resume_analyzer.py*
class ResumeAnalyzer:
    """Class to analyze resumes using GroqHandler."""
    def __init__(self, groq_handler, prompt_loader):
        self.grok = groq_handler
        self.prompt_loader = prompt_loader

    def analyze_resume(self, text, designation, experience, domain):
        """Analyze resume text."""
        prompt = self.prompt_loader.get_prompt(
            "resume_analysis",
            designation=designation,
            experience=experience,
            domain=domain
        )
        result = self.grok.analyze_text(prompt, text, max_tokens=1500)
        return result
    
*Update main/app.py*
import sys
import os
import streamlit as st

# Add the project root directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from lib.groq_handler import GroqHandler
from lib.text_processor import TextProcessor
from lib.resume_analyzer import ResumeAnalyzer
from utils.config import Config
from utils.prompt_loader import PromptLoader

def main():
    """Main function to run the Streamlit Resume Analyzer app."""
    st.title("Resume Analyzer")
    st.subheader("Upload a PDF or Word Resume")
    uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])
    col1, col2, col3 = st.columns(3)
    with col1:
        designation = st.selectbox("Select Desired Designation", ["Data Scientist", "Data Analyst", "MLOps Engineer", "Machine Learning Engineer"])
    with col2:
        experience = st.selectbox("Select Experience Level", ["Fresher", "<1 Year Experience", "1-2 Years Experience", "2-5 Years Experience", "5-8 Years Experience", "8-10 Years Experience"])
    with col3:
        domain = st.selectbox("Select Domain", ["Finance", "Healthcare", "Automobile", "Real Estate"])
    if uploaded_file and st.button("Analyze"):
        file_extension = uploaded_file.name.split(".")[-1].lower()
        temp_path = f"temp_resume.{file_extension}"
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        text_processor = TextProcessor()
        extracted_text = text_processor.extract_text(temp_path, file_extension)
        os.remove(temp_path)
        if extracted_text:
            grok_handler = GroqHandler()
            prompt_loader = PromptLoader(Config.PROMPTS_FILE)
            resume_analyzer = ResumeAnalyzer(grok_handler, prompt_loader)
            with st.spinner("Analyzing resume... Please wait"):
                analysis = resume_analyzer.analyze_resume(extracted_text, designation, experience, domain)
            st.markdown("# Resume Analysis")
            st.write(analysis)
        else:
            st.error("Could not extract text.")

if __name__ == "__main__":
    main()

*Test*
    Run streamlit run main/app.py and test with dropdown selections.

*Commit*
- git add utils/config.py data/prompts.json utils/prompt_loader.py lib/resume_analyzer.py main/app.py
- git commit -m "Add structured analysis with prompts and update Config"
- git push origin main